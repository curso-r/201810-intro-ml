---
title: "Regressão logística"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Regressão vs Classificação

Regressão: queremos prever uma variável numérica
Classificação: queremos prever uma variável categórica

O que muda?


- Estimação
- Métrica de performance

# Regressão logística

Seja Y uma variável que pode assumir duas categorias.

Por exemplo: Y = 1 (bom cliente), Y = 0 (mau cliente)

O que queremos é modelar 

$$
P(Y = 1| X)
$$

Então

$$
P(Y = 1| X) \approx \hat{f}(X)
$$



Na regressão linear:

$$
f(x) = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_px_p
$$

> Por que não podemos usar essa mesma expressão para uma variável categórica?

Vamos usar então

$$
f(x) = \frac{1}{1 + e^{-( \beta_0 +  \beta_1*x_1 + \beta_2*x_2 + ... + \beta_p * x_p)}}
$$

Equivalentemente

$$
P(Y = 1| X) \approx \frac{1}{1 + e^{-( \beta_0 +  \beta_1*x_1 + \beta_2*x_2 + ... + \beta_p * x_p)}}
$$
e

$$
\log \left(\frac{P(Y = 1| X)}{1 - P(Y = 1| X)}\right) \approx \beta_0 +  \beta_1*x_1 + \beta_2*x_2 + ... + \beta_p * x_p
$$

Minimizar 

$$
L(y, \hat{f}(x))
$$

Antes, tínhamos

$$
L(y, f(x)) = \sum(y - f(x))^2
$$

Agora, queremos maximizar

$$
\prod_{i:Y_i = 1} P(Y_i=1|X_i) \prod_{i:Y_i = 0} (1 - P(Y_i=1|X_i))
$$

Portanto, minimizar

$$
L(y, f(x)) = - \prod_{i:Y_i = 1} P(Y_i=1|X_i) \prod_{i:Y_i = 0} (1 - P(Y_i=1|X_i))
$$

Previsões: probabilidade estimada de P(Y = 1|X). Precisamos definir então quando Y = 1 e Y = 0.

# Pacotes

```{r}
library(dplyr)
library(recipes)
library(caret)
library(rsample)
library(skimr)
```

# Banco de dados

```{r}
data("credit_data")
glimpse(credit_data)
```

```{r}
ggplot(credit_data, aes(x = Status)) + 
  geom_bar()

skim(credit_data)

credit_data %>% 
  group_by(Status) %>% 
  skim()
```

# Ajustando modelos

```{r}
receita <- recipe(Status ~ ., data = credit_data) %>%
  step_meanimpute(all_numeric(), -all_outcomes()) %>%
  step_modeimpute(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_corr(all_predictors()) %>%
  step_nzv(all_predictors())

receita

modelo <- train(
  receita, 
  credit_data, 
  method = "glm", 
  family = "binomial", 
  trControl = trainControl(method = "cv", number = 5)
)
modelo
```

```{r}
metricas <- function(data, lev = NULL, model = NULL) {
  c(
    defaultSummary(data, lev, model), 
    twoClassSummary(data, lev, model)
  )
}

train_control <- trainControl(
  method = "cv", 
  number = 5, 
  classProbs = TRUE,
  summaryFunction = metricas
)
```

```{r}
modelo <- train(
  receita, 
  credit_data, 
  method = "glm", 
  family = "binomial", 
  trControl = train_control
)

modelo
summary(modelo)
cor(credit_data %>% select_if(is.numeric), use = "complete.obs")
```

# Regularização

- Penalizar os coeficientes no processo de estimação
- Encolher os coeficientes na direção do zero 
- Diminui a variância do modelo em troca de um pouco de viés

Regressão ridge

$$
L(y, f(x)) - \lambda\sum_{i=1}^pb_i^2
$$

LASSO

$$
L(y, f(x)) - \lambda\sum_{i=1}^p|b_i|
$$

Podemos reescrever de outra forma:

$$
\underset{\beta}{\text{minimizar}}L(y, f(x)) \text{ sujeito à } \sum_{j=1}^p \beta_j^2 \leq s \text{ ou } \sum_{j=1}^p |\beta_j| \leq s
$$

```{r}
knitr::include_graphics("lasso_ridge.png")
```


```{r}
receita <- recipe(Status ~ ., data = credit_data) %>%
  step_meanimpute(all_numeric(), -all_outcomes()) %>%
  step_modeimpute(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

modelo <- train(
  receita, 
  credit_data, 
  method = "glmnet", 
  family = "binomial", 
  trControl = train_control, 
  metric = "ROC"
)

modelo
modelo$bestTune
coef(modelo$finalModel, s = modelo$bestTune$lambda)
```

```{r}
tune_grid <- expand.grid(alpha = c(0, 0.55, 1), lambda = 10^(-6:0))

modelo <- train(
  receita, 
  credit_data, 
  method = "glmnet", 
  family = "binomial", 
  trControl = train_control, 
  metric = "ROC", 
  tuneGrid = tune_grid
)

modelo
modelo$bestTune
coef(modelo$finalModel, s = modelo$bestTune$lambda)
plot(modelo)
varImp(modelo)
```


# Exercício

Ajuste um modelo de regressão logística e um de LASSO p/ prever o churn de funcionários no banco de dados `attrtion`.

```{r}
library(rsample)
data(attrition, package = "rsample")
glimpse(attrition)
```

```{r}

```
