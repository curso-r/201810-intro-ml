---
title: "Machine Learning com R"
subtitle: "Outros Tópicos"
author: "@Curso-R"
date: "23-05-2018"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "default-fonts", "custom.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: js/macro.js
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  warning = FALSE,
  fig.width=6, 
  fig.height=6,
  fig.align='center'
)

library(tidyverse)
```

# Funções de perda

Queremos encontrar funções $f$ tais que

$$Y \approx f(X)$$

--

Para isso, definimos uma função de perda $L(y, f)$ e encontramos a função $\hat{f}$ que minimiza essa função.

--

- Mínimos quadrados

$$\sum_{i=1}^n \left(y_i - f(x_i)\right)^2 = \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2$$

--

- Verossimilhança

$$- f(y_1, \dots, y_n) \stackrel{\text{iid}}{=} - \prod_{i=1}^{n}f(y_i)$$

--

Por que apenas a minimização da função de perda não garante que a $\hat{f}$ obtida seja uma boa escolha?

---

# Regularização

- Regularização se refere à técnica de incluir penalizações na função de perda que encolhem as estimativas na direção do zero.

--

- Essa técnica pode levar a uma redução substancial da variância das predições ao custo de um pequeno aumento de viés.

--

Vamos falar de:

- Regressão Ridge

- LASSO

---

# Regressão Ridge

Regularização considerando estimação por mínimos quadrados.

$$L(y, f) = \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^p \beta_j^2$$

--

- O hiperparâmetro $\lambda$ controla o impacto da penalização nas estimativas dos parâmetros.

--

- O que acontece quando $\lambda \longrightarrow 0$?

--

- O que acontece quando $\lambda \longrightarrow \infty$?

--

- As estimativas obtidas minimizando essa expressão não serão invariantes à escala.

--

- $\beta_i = 0$ apenas se $\lambda = \infty$.

--

- Vamos ter um conjunto de estimativas diferentes para cada valor de $\lambda$.

---

# LASSO

(least absolute shrinkage and selection operator)


$$\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^p |\beta_j|$$

--

- Para $\lambda$ suficientemente grande, alguns parâmetros serão estimados exatamente como 0.

--

- O que acontece quando $\lambda \longrightarrow 0$? E quando $\lambda \longrightarrow \infty$?

---

# Outra formulação

$$\underset{\beta}{\text{minimizar}}\left\{\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2\right\} \text{ sujeito à } \sum_{j=1}^p \beta_j^2 \leq s \text{ ou } \sum_{j=1}^p |\beta_j| \leq s$$
--

```{r}
knitr::include_graphics("img/lasso_ridge.png")
```

---

## Exemplo

```{r echo=TRUE}
library(caret)
library(ggplot2)

train_control <- trainControl(method = "cv", number = 5)
model <- train(price ~., data = diamonds, method = "glmnet", 
               trControl = train_control)
```

---

## Elastic Net

$$\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 + \alpha(\lambda \sum_{j=1}^p |\beta_j|) + (1-\alpha)(\lambda \sum_{j=1}^p \beta_j ^2)$$

---

## Exercício: Faça CV para encontrar o melhor LASSO

**Dica**: lembre como faz para passar hiper-parâmetros pré-definidos para o caret.

```{r echo=TRUE}

```

---

## Ensemble learning

Usa muitos modelos para obter poder preditivo maior do que se fosse usado apenas um dos modelos.

### Principais Métodos

* Bagging
* Boosting
* Stacking

---

## Bagging

* Pega A amostras com reposição da base de treino
* Para cada amostra "a"" em "A" treinamos um modelo
* No final fazemos a média das predições de todos os modelos

![](https://upload.wikimedia.org/wikipedia/en/d/de/Ozone.png)

---

## Random Forests

* Pega A amostras com reposição da base de treino
* A base de treino possui $p$ variáveis
* Para cada amostra $a$ em $A$ selecione aleatóriamente um conjunto de tamanho $m < p$ das variáveis
* Para cada amostra $a$ em $A$ treine uma árvore de decisão com as $m$ variáveis selecionadas.
* Combine todas as árvores fazendo a média das previsões individuais.

---

## Boosting

* Inicie o modelo como $\hat{f}(x) = 0$ e $r_i = y_i$ para todo $i$ na base de treino.
* Considere B o número de modelos que você vai fazer.
    * Ajuste o modelo $b$ para a base de treino e use como respota o valor $r$. 
    * Atualize $\hat{f}(x)$ da seguinte forma: $\hat{f}(x) = \hat{f}(x) + \lambda\hat{f_b}(x)$
    * Atualize os resíduos $r_i$ = $r_{i} - \lambda\hat{f_b}(x) 
* O seu modelo será:

$$\hat{f}(x) = \sum_{b = 1}^{B} \lambda \hat{f_b}(x)$$

---

## Random Forest

```{r, echo = TRUE, eval = FALSE}
train_control <- trainControl(method = "cv", number = 5)
model <- train(price ~., dplyr::sample_n(diamonds,1000), method = "ranger",
               trControl = train_control)
```

---

## Boosting

```{r, echo = TRUE, eval = FALSE}
train_control <- trainControl(method = "cv", number = 5)
model <- train(price ~., dplyr::sample_n(diamonds,1000), method = "gbm",
               trControl = train_control)
```




